{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Partial Derivatives & Gradients\n",
        "## Calculus for Machine Learning\n",
        "\n",
        "This notebook covers:\n",
        "- Partial derivatives in multiple dimensions\n",
        "- Gradient as vector of partial derivatives\n",
        "- Direction of steepest ascent\n",
        "- Gradient visualization\n",
        "- Applications in machine learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding Partial Derivatives\n",
        "\n",
        "Given our error function:\n",
        "$$\\text{Error}(w,b) = (w \\cdot x + b - y)^2$$\n",
        "\n",
        "To make it simpler, let's pretend $x=2$ and $y=5$. So now we have:\n",
        "$$f(w,b) = (2w + b - 5)^2$$\n",
        "\n",
        "Let's find the partial derivatives with respect to $w$ and $b$.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partial Derivative with respect to b\n",
        "\n",
        "Using the chain rule:\n",
        "$$\\frac{\\partial f}{\\partial b} = \\frac{\\partial}{\\partial b}[(2w + b - 5)^2]$$\n",
        "\n",
        "Let $u = 2w + b - 5$, then $f = u^2$\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial b} = \\frac{\\partial f}{\\partial u} \\cdot \\frac{\\partial u}{\\partial b}$$\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial u} = 2u = 2(2w + b - 5)$$\n",
        "\n",
        "$$\\frac{\\partial u}{\\partial b} = 1$$\n",
        "\n",
        "Therefore:\n",
        "$$\\frac{\\partial f}{\\partial b} = 2(2w + b - 5) \\cdot 1 = 2(2w + b - 5)$$\n",
        "\n",
        "**Result:** $\\boxed{\\frac{\\partial f}{\\partial b} = 2(2w + b - 5)}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Vector (∇f)\n",
        "\n",
        "The **gradient** is a vector that contains all the partial derivatives of a function. It's denoted by the nabla symbol ∇ (del).\n",
        "\n",
        "For our function $f(w,b) = (2w + b - 5)^2$, the gradient is:\n",
        "\n",
        "$$\\nabla f(w,b) = \\begin{bmatrix} \\frac{\\partial f}{\\partial w} \\\\ \\frac{\\partial f}{\\partial b} \\end{bmatrix}$$\n",
        "\n",
        "Substituting our calculated partial derivatives:\n",
        "\n",
        "$$\\nabla f(w,b) = \\begin{bmatrix} 4(2w + b - 5) \\\\ 2(2w + b - 5) \\end{bmatrix}$$\n",
        "\n",
        "We can factor out the common term $2(2w + b - 5)$:\n",
        "\n",
        "$$\\boxed{\\nabla f(w,b) = 2(2w + b - 5) \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}}$$\n",
        "\n",
        "### General Form\n",
        "\n",
        "For the general error function $\\text{Error}(w,b) = (wx + b - y)^2$:\n",
        "\n",
        "$$\\nabla \\text{Error}(w,b) = \\begin{bmatrix} \\frac{\\partial \\text{Error}}{\\partial w} \\\\ \\frac{\\partial \\text{Error}}{\\partial b} \\end{bmatrix} = \\begin{bmatrix} 2x(wx + b - y) \\\\ 2(wx + b - y) \\end{bmatrix}$$\n",
        "\n",
        "$$\\boxed{\\nabla \\text{Error}(w,b) = 2(wx + b - y) \\begin{bmatrix} x \\\\ 1 \\end{bmatrix}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aiml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
