{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimization\n",
        "## Calculus for Machine Learning\n",
        "\n",
        "This notebook covers:\n",
        "- Finding function minima\n",
        "- Moving opposite to gradient direction\n",
        "- Local vs global minima\n",
        "- Optimization algorithms overview\n",
        "- Connection to machine learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid being stuck in a local minimum from just 'walking down' you can use Momentum (algos such as Adam) or randomness (Stochastic Gradient Descent). \n",
        "\n",
        "**Goal** Find model parameters (weights) that result in lowest possible value of our loss function (finding the minimum)\n",
        "\n",
        "**Tool** Gradient of the loss function- tells us for every weight which direction would make the loss increase fastest\n",
        "\n",
        "**Strategy** Update all weights by taking a small step in the opposite direction of gradient (descent)\n",
        "\n",
        "**Challenge** Not getting stuck in puddle/local minimums to find the global minimum"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aiml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
