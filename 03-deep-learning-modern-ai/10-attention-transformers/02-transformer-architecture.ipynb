{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Architecture\n",
        "## Attention and Transformers\n",
        "\n",
        "This notebook covers:\n",
        "- Self-attention mechanism\n",
        "- Multi-head attention\n",
        "- Positional encodings\n",
        "- Encoder-decoder structure\n",
        "- Layer normalization and residual connections\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
